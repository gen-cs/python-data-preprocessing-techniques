# ğŸ§¹ Data Cleaning & âš™ï¸ Feature Engineering: Basic â†’ Advanced

> **"80% of your ML project time is spent here â€” and for good reason."**  
This repository is a **complete Python-powered toolkit** for preparing raw data and transforming it into **machine-learning-ready features**.  
From **basic cleaning** to **advanced automated feature engineering**, this guide covers the techniques you need to turn messy datasets into predictive gold.

---

## ğŸ§¼ Data Cleaning Techniques

### **ğŸ“ Basic Data Cleaning**
#### ğŸ§© Missing Data Handling
- **Removal** â†’ Delete observations with missing values  
- **Imputation** â†’ Fill missing values with **mean**, **median**, **mode**, or statistical methods  
- **Forward / Backward Fill** â†’ For time series data  
- **Interpolation** â†’ Linear / polynomial interpolation for continuous variables  

#### ğŸ” Duplicate Detection & Removal
- Identify identical or near-identical records  
- Remove exact duplicates while preserving unique info  
- Handle slight differences from data entry errors  

#### ğŸ“Š Outlier Management
- **Detection** â†’ IQR, Z-score, visualizations  
- **Treatment** â†’ Remove, transform, or cap outliers  
- **Contextual Assessment** â†’ Decide if they are valid extremes or errors  

#### ğŸ›  Structural Error Correction
- Fix inconsistencies in formats and variable types  
- Standardize naming conventions & capitalization  
- Correct typos & inconsistent labels  

---

### **ğŸš€ Advanced Data Cleaning**
#### ğŸ”‡ Noise Reduction
- **Binning** â†’ Group data points, smooth with bin means  
- **Regression Smoothing** â†’ Fit regression functions  
- **Clustering** â†’ Detect and handle anomalies outside cluster bounds  

#### âš ï¸ Inconsistency Detection
- **Pointwise** â†’ Values outside expected ranges (negative ages, impossible dates)  
- **Multi-variable** â†’ Conflicting info across features  
- **Domain-Specific** â†’ Apply business rules & constraints  

---

## âš™ï¸ Feature Engineering Techniques

### **ğŸ”° Basic Feature Engineering**
#### ğŸ· Categorical Encoding
- **Label Encoding** â†’ Assign numeric IDs to categories  
- **One-Hot Encoding** â†’ Binary columns per category  
- **Binary Encoding** â†’ Use binary digits  
- **Target Encoding** â†’ Encode by target variable statistics  

#### ğŸ”¢ Numerical Transformations
- **Log / Power Transformations** â†’ Fix skewness  
- **Scaling & Normalization**:  
  - Min-Max Scaling (0â€“1 range)  
  - Standardization (zero mean, unit variance)  
  - Robust Scaling (median, IQR â€” outlier resistant)  

#### â³ Date-Time Feature Extraction
- Extract **day, month, year, hour, weekday**  
- Create **cyclical features** (sin/cos for periodic patterns)  
- Calculate **time deltas** & elapsed periods  

---

### **ğŸ“ˆ Intermediate Feature Engineering**
#### ğŸ“¦ Binning & Discretization
- Equal-width intervals  
- Equal-frequency bins  
- Domain-knowledge custom bins  

#### ğŸ”– Boolean / Flag Features
- Convert conditions into binary indicators  
- Threshold-based anomaly flags  
- Status indicators via business rules  

#### â— Feature Interactions
- Multiplication, addition, subtraction  
- Ratios, percentages, proportions  

---

### **ğŸ’ Advanced Feature Engineering**
#### ğŸ“ Polynomial Features
- Higher-order terms (xÂ², xÂ³), cross-products  
- Capture non-linear relationships (beware overfitting)  

#### ğŸ“Š Statistical Features
- Rolling statistics (moving averages, rolling std)  
- Group aggregations (mean, max, min, std)  
- Lag features for time series  

#### ğŸ¯ Domain-Specific
- **Text Data** â†’ TF-IDF, embeddings, n-grams  
- **Time Series** â†’ Seasonal decomposition, FFT features  
- **Image Data** â†’ Texture, edges, color histograms  

---

## ğŸ§  Feature Selection Techniques

### ğŸ§® Filter Methods
- Univariate tests: Chi-square, F-test, mutual info  
- Correlation-based: Pearson, Spearman  
- Variance thresholding (remove low-variance features)  

### ğŸ” Wrapper Methods
- Forward selection  
- Backward elimination  
- Recursive Feature Elimination (RFE)  

### ğŸŒ² Embedded Methods
- L1 Regularization (Lasso)  
- Tree-based feature importance  
- Gradient boosting importance  

---

## ğŸ“‰ Dimensionality Reduction Techniques

### ğŸ“ Linear
- PCA â†’ Maximize variance in fewer dimensions  
- LDA â†’ Maximize class separation  
- Factor Analysis â†’ Extract latent factors  

### ğŸ”€ Non-linear
- t-SNE â†’ Preserve local structure in lower-dim space  
- UMAP â†’ Uniform manifold approximation  
- Autoencoders â†’ Neural-based compression  

---

## ğŸ¤– Automated Feature Engineering

### ğŸ“¦ Popular Tools
- **Featuretools** â†’ Deep Feature Synthesis (DFS)  
- **AutoFeat** â†’ Non-linear features + L1 selection  
- **TSFresh** â†’ 60+ time-series features  
- **Feature Engine** â†’ Advanced encoders, correlation filters  

### âœ… Benefits
- **Efficiency** â†’ Scale across projects  
- **Bias Reduction** â†’ Avoid human preconceptions  
- **Consistency** â†’ Standardize approach  
- **Exploration** â†’ Find hidden relationships  

---

## ğŸ“ Text Data Preprocessing & Feature Engineering

### ğŸ§¼ Basic Text Cleaning
- Tokenization, stop word removal  
- Lowercasing, punctuation removal  

### ğŸ” Advanced Text Processing
- Stemming, lemmatization  
- N-grams, POS tagging  
- Named Entity Recognition (NER)  

### ğŸ”¡ Text Feature Engineering
- TF-IDF  
- Word embeddings (Word2Vec, GloVe, FastText)  
- Sentence embeddings  
- Bag of Words (BoW)  

---

## ğŸ’¡ Best Practices

- ğŸ“Œ **Algorithm-dependent preprocessing** â€” tree models need less scaling, distance-based models need more  
- ğŸ”’ **Prevent data leakage** â€” respect train/validation/test boundaries  
- ğŸ“Š **Validate impact** â€” cross-validate preprocessing effects  
- âš¡ **Balance performance & efficiency** â€” avoid over-engineering  

---

## ğŸ¯ Final Thoughts
This **progression from basic cleaning to advanced automated feature engineering** is your complete data preprocessing arsenal.  
The techniques you choose will depend on:
- ğŸ“‚ Data characteristics  
- ğŸ­ Problem domain  
- ğŸ–¥ Computational resources  
- ğŸ§  Algorithm choice  

---

ğŸš€ **Turn raw data into predictive power â€” one feature at a time.**
